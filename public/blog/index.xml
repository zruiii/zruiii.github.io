<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Ray’Log</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content in Blogs on Ray’Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Tue, 11 Feb 2025 16:57:10 +0800</lastBuildDate><atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flow Matching</title>
      <link>http://localhost:1313/blog/flow-matching/</link>
      <pubDate>Tue, 11 Feb 2025 16:57:10 +0800</pubDate>
      
      <guid>http://localhost:1313/blog/flow-matching/</guid>
      <description>&lt;p&gt;生成模型是深度学习中的重要研究方向，其核心目标是学习复杂的数据分布并生成新样本。在过去的几年中，从VAE、GAN到Diffusion模型，研究者们提出了多种生成模型范式。本文将介绍一个新兴的生成模型方法：Flow Matching，并从其理论发展脉络出发，展现这一方法的独特优势。&lt;/p&gt;
&lt;p&gt;在开始之前，让我们通过一个简单的例子来理解Flow Matching的核心思想：想象我们有一团云（简单的高斯分布），我们希望将它逐渐变形成一只猫（复杂的数据分布）。传统的方法可能需要精确计算这个变形过程中的概率变化，而Flow Matching则提供了一种更直观的方式：直接学习&amp;quot;云&amp;quot;变成&amp;quot;猫&amp;quot;的运动轨迹，就像给每个点标注一个&amp;quot;速度向量&amp;quot;，告诉它该往哪个方向移动。&lt;/p&gt;
&lt;p&gt;本文将按以下框架展开讨论：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先介绍Normalizing Flow的基本原理及其局限性&lt;/li&gt;
&lt;li&gt;然后探讨Flow Matching的动机和核心概念&lt;/li&gt;
&lt;li&gt;最后对比Flow Matching与Diffusion Model的异同&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE: 引言由 Claude 生成。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;1-normalization-flow&#34;&gt;1. Normalization Flow&lt;/h2&gt;
&lt;h3 id=&#34;11-动机&#34;&gt;1.1 动机&lt;/h3&gt;
&lt;p&gt;假设真实样本服从某个未知分布 $p_{\text{data}}(x)$，我们观测到的样本均属于该分布。理论上，从该分布中采样可以生成未观测到的数据（即合成样本）。VAE 和 GAN 这类生成模型的核心思想正是基于观测数据建模真实分布：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VAE&lt;/strong&gt; 通过变分推断逼近真实分布，但依赖近似后验分布，可能导致生成样本模糊；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GAN&lt;/strong&gt; 通过对抗训练隐式学习分布，但无法直接计算似然，且训练不稳定。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为此，Normalizing Flow提出了一种新的思路：通过设计一系列可逆变换，将简单的初始分布（如高斯分布）精确地映射到复杂的数据分布。&lt;strong&gt;这种方法的优势在于可以直接计算似然，从而通过极大似然估计优化模型。&lt;/strong&gt;&lt;/p&gt;
&lt;!-- &lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/image-20250126190244052.png&#34; alt=&#34;image-20250126190244052&#34; style=&#34;zoom: 25%;&#34; /&gt; --&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/image-20250126190244052.png&#34; 
alt=&#34;image-20250126190244052&#34; 
style=&#34;zoom: 25%; display: block; margin-left: auto; margin-right: auto;&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-方法&#34;&gt;1.2 方法&lt;/h3&gt;
&lt;h4 id=&#34;121-变量变换公式&#34;&gt;1.2.1 变量变换公式&lt;/h4&gt;
&lt;p&gt;根据概率密度守恒定律，若 $x = G_{\theta}(z)$ 是可逆变换且 $G_{\theta}$ 可微，则对任意区域 $\mathcal{Z}$ 和其映射区域 $\mathcal{X} = G_{\theta}(\mathcal{Z})$，有：&lt;br&gt;
&lt;/p&gt;
$$
\int_{\mathcal{Z}} p_{\text{base}}(z) dz = \int_{\mathcal{X}} p_{\theta}(x) dx.
$$
&lt;p&gt;
结合变量变换定理，可推导出生成分布密度的显式表达式：&lt;br&gt;
&lt;/p&gt;
$$
p_{\theta}(x) = p_{\text{base}}\left( G_{\theta}^{-1}(x) \right) \cdot \left| \det J_{G_{\theta}^{-1}}(x) \right|,
$$
&lt;p&gt;
其中 $J_{G_{\theta}^{-1}}(x) = \frac{\partial G_{\theta}^{-1}(x)}{\partial x}$ 是逆变换的雅可比矩阵。&lt;/p&gt;
&lt;p&gt;这个公式揭示了一个重要的性质：&lt;strong&gt;只要我们的生成器 $G_{\theta}$​ 是可逆的，并且我们可以计算出它的雅可比矩阵，那么就可以精确地计算出生成分布的概率密度。&lt;/strong&gt; 这意味着我们可以直接采用极大似然估计来评估模型生成的样本质量，而不需要像GAN那样依赖判别器的反馈。&lt;/p&gt;
&lt;h4 id=&#34;122-极大似然估计&#34;&gt;1.2.2 极大似然估计&lt;/h4&gt;
&lt;p&gt;模型通过最小化负对数似然损失进行优化：
&lt;/p&gt;
$$
\mathcal{L}(\theta) = -\mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ \log p_{\theta}(x) \right].
$$
&lt;p&gt;
将公式(2)代入并展开：&lt;br&gt;
&lt;/p&gt;
$$
\mathcal{L}(\theta) = -\mathbb{E}_{x} \left[ \log p_{\text{base}}\left( G_{\theta}^{-1}(x) \right) + \log \left| \det J_{G_{\theta}^{-1}}(x) \right| \right].
$$
&lt;p&gt;
进一步利用 $z = G_{\theta}^{-1}(x)$ 的关系，可等价表示为对隐变量 $z$ 的优化：&lt;br&gt;
&lt;/p&gt;
$$
\mathcal{L}(\theta) = -\mathbb{E}_{z \sim p_{\text{base}}(z)} \left[ \log p_{\text{base}}(z) - \log \left| \det J_{G_{\theta}}(z) \right| \right].
$$
&lt;h4 id=&#34;123-nf-的局限性&#34;&gt;1.2.3 NF 的局限性&lt;/h4&gt;
&lt;p&gt;Normalizing Flow 虽然理论优美，但存在几个关键限制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成器必须严格可逆，这限制了模型的表达能力&lt;/li&gt;
&lt;li&gt;雅可比行列式的计算复杂度高，特别是在高维数据上&lt;/li&gt;
&lt;li&gt;对生成器结构的约束可能影响生成质量&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13-连续正则化流cnf&#34;&gt;1.3 连续正则化流（CNF）&lt;/h3&gt;
&lt;p&gt;正则化流需要设计一系列离散的可逆变换来实现分布转换。连续正则化流 (Continuous Normalizing Flow, CNF) 则提供了一个优雅的推广：将离散变换扩展到连续时间域，通过常微分方程(ODE)来描述分布的演化过程。&lt;/p&gt;
&lt;p&gt;CNF的核心思想是将分布的变换看作是一个连续的动力学系统。在这个系统中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个数据点都遵循一条由神经网络定义的轨迹&lt;/li&gt;
&lt;li&gt;轨迹由向量场驱动（类似物理中的速度场）&lt;/li&gt;
&lt;li&gt;整体分布的演化满足概率守恒定律&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;连续性方程&#34;&gt;连续性方程&lt;/h5&gt;
&lt;p&gt;概率密度演化遵循连续性方程：
&lt;/p&gt;
$$
\frac{\partial p_t}{\partial t} + \nabla \cdot (p_t \cdot u_t) = 0
$$
&lt;p&gt;
其中 $p_t$ 是时间 t 处的概率密度，$u_t$ 是向量场。&lt;/p&gt;
&lt;p&gt;该方程的物理意义在于：&lt;strong&gt;概率密度的局部变化率（时间导数）与概率流的净流出率（散度）之和为零，确保概率守恒。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其物理意义类似于人群流动：如果把 $p_t(x)$ 看作人群密度，$u_t(x)$ 看作移动速度，那么 $p_t(x) \cdot u_t(x)$ 就表示单位时间内通过某点的人流量。&lt;/p&gt;
&lt;h5 id=&#34;似然计算&#34;&gt;似然计算&lt;/h5&gt;
&lt;p&gt;基于连续性方程，我们可以推导出概率密度的对数似然：
&lt;/p&gt;
$$
log p_θ(x) = log p_{base}(z) + ∫_{t=0}^1 -div(u_t(x_t, θ)) dt \\
z = x - ∫_{t=0}^1 u_t(x_t, θ)dt
$$
&lt;p&gt;
然而，模型中包含了从 t=0 到 t=1 的积分项，这种积分在实践中无法直接计算，需要使用常微分方程(ODE)求解器来数值求解。&lt;/p&gt;
&lt;p&gt;ODE求解器在计算过程中需要多次评估矢量场($u_t$)，每次评估都涉及到复杂的计算。当数据规模变大时，计算成本会显著增加。&lt;/p&gt;
&lt;p&gt;因此，这限制了模型在实际大规模应用中的使用，进而引出了 Flow Matching 的出现。&lt;/p&gt;
&lt;h2 id=&#34;2-flow-matching&#34;&gt;2. Flow Matching&lt;/h2&gt;
&lt;h3 id=&#34;21-动机&#34;&gt;2.1 动机&lt;/h3&gt;
&lt;p&gt;Flow Matching 的提出源于一个关键洞察：我们是否必须显式计算概率密度的变化？如果我们能直接学习数据点是如何&amp;quot;移动&amp;quot;的，是否可以绕过传统方法的限制？&lt;/p&gt;
&lt;p&gt;这种思路启发自物理学中的流体动力学：与其追踪水的密度变化，不如直接描述水的流动方向和速度。这就是Flow Matching的核心思想：&lt;strong&gt;学习一个描述数据如何演化的向量场。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-方法&#34;&gt;2.2 方法&lt;/h3&gt;
&lt;h4 id=&#34;221-流匹配fm&#34;&gt;2.2.1 流匹配（FM）&lt;/h4&gt;
&lt;p&gt;不同于正则化流通过最大似然来学习分布转换，流匹配提出直接学习一个矢量场，该矢量场定义了从数据分布到目标分布的演化路径。&lt;/p&gt;
&lt;p&gt;流匹配的核心思想是：每个观测样本都可以从一个简单分布（如高斯分布）通过连续演化得到。这种演化由向量场控制，因此优化目标转化为最小化向量场的差异：
&lt;/p&gt;
$$
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t,p_t(x_t)} \left\| v_t(x_t, \theta) - u_t(x_t) \right\|^2
$$
&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$v_t(x_t, \theta)$：神经网络建模的向量场（可学习参数 $\theta$）&lt;/li&gt;
&lt;li&gt;$u_t(x_t)$：生成目标路径 $p_t(x)$ 的理想向量场（未知）&lt;/li&gt;
&lt;li&gt;$t \sim \mathcal{U}[0,1]$​：时间均匀采样&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然而，直接求解这个优化目标非常困难。首先，我们并不知道概率流 $p_t(x)$ 和向量场 $u_t(x)$​ 的值。其次，FM 的损失函数计算涉及到高维积分，计算起来十分复杂。为此，Flow Matching 进一步提出了条件流匹配的解决策略。&lt;/p&gt;
&lt;h4 id=&#34;222-条件流匹配cfm&#34;&gt;2.2.2 条件流匹配（CFM）&lt;/h4&gt;
&lt;p&gt;为解决直接优化 FM 损失函数的困难，条件流匹配（CFM）将全局问题分解为样本级路径。即不要试图同时学习所有数据点的运动，而是为每个数据点分别学习一条路径。&lt;/p&gt;
&lt;p&gt;具体而言：对于每个真实数据点 $x_1$​，我们定义一条从简单分布到该点的路径；通过优化每条路径上的向量场，最终得到整体的生成模型。下面我们来推导这个方法：&lt;/p&gt;
&lt;h5 id=&#34;边际向量场&#34;&gt;边际向量场&lt;/h5&gt;
&lt;p&gt;结合贝叶斯定理，我们可以推导&lt;strong&gt;边际向量场&lt;/strong&gt;：
&lt;/p&gt;
$$
\begin{aligned}
u_t(x) &amp;= \int u_t(x|x_1)p(x_1|x)dx_1 \\
&amp;= \int u_t(x|x_1) \frac{p_t(x|x_1) q(x_1)}{p_t(x)} dx_1
\end{aligned}
$$
&lt;p&gt;&lt;strong&gt;CFM 优化目标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;回顾Flow Matching的损失函数：
&lt;/p&gt;
$$
\begin{aligned}
\mathcal{L}_{\text{FM}}(\theta) &amp;= \mathbb{E}_{t,p_t(x)} \left\| v_t(x) - u_t(x) \right\|^2 \\
&amp;= \mathbb{E}_{t,p_t(x)} [\|v_t(x)\|^2 - 2 \cdot v_t(x)u_t(x) + \|u_t(x)\|^2] \\
\end{aligned}
$$
&lt;p&gt;代入边际向量场表达式，我们可以得到：
&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}_{x_t \sim p_t(x)}[2 \cdot v_t(x)u_t(x)] &amp;= 2 \cdot \int v_t(x_t) \cdot \frac{\int u_t(x_t|x_1)p_t(x_t|x_1) q(x_1)dx_1}{p_t(x_t)} p_t(x_t)dx_t \\
&amp;= 2 \cdot \int \int v_t(x) u_t(x_t|x_1)p_t(x_t|x_1) q(x_1) dx_1 dx_t \\
&amp;= 2 \cdot \mathbb{E}_{x_t \sim p_t(x_t|x_1), x_1 \sim q(x_1)} [v_t(x_t) \cdot u_t(x_t|x_1)]
\end{aligned} \\
$$
&lt;p&gt;因此，我们可以将 FM 的损失函数转换为条件形式：
&lt;/p&gt;
$$
\begin{aligned}
\mathcal{L}_{\text{FM}}(\theta) &amp;= \mathbb{E}_{x_t \sim p_t(x_t|x_1), x_1 \sim q(x_1)} [\|v_t(x)\|^2 - 2 \cdot v_t(x_t)u_t(x_t|x_1) + \|u_t(x_t|x_1)\|^2 + \|u_t(x_t)\|^2 - \|u_t(x_t|x_1)\|^2 ] \\
&amp;= \mathbb{E}_{x_t \sim p_t(x_t|x_1), x_1 \sim q(x_1)} \left\| v_t(x) - u_t(x|x_1) \right\|^2 \\
&amp;+ \mathbb{E}_{x_t \sim p_t(x_t|x_1), x_1 \sim q(x_1)} [\|u_x(x_t)\|^2]\\
&amp;+ \mathbb{E}_{x_t \sim p_t(x_t|x_1), x_1 \sim q(x_1)} [\|u_t(x_t|x_1)\|^2]
\end{aligned}
$$
&lt;p&gt;因为后面两项并不涉及我们要优化的网络参数 $v_t(x_t, \theta)$，因此可以舍弃。&lt;/p&gt;
&lt;p&gt;这就得到了 CFM 的最终优化目标：
&lt;/p&gt;
$$
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t, q(x_1), p_t(x|x_1)} \left\| v_t(x) - u_t(x|x_1) \right\|^2
$$
&lt;p&gt;这个目标函数的优势在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;避免了计算复杂的边际分布&lt;/li&gt;
&lt;li&gt;可以针对每个数据点独立优化路径&lt;/li&gt;
&lt;li&gt;训练过程更加稳定和高效&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;223-cfm-中的训练采样策略&#34;&gt;2.2.3 CFM 中的训练采样策略&lt;/h4&gt;
&lt;p&gt;在CFM的训练过程中，我们需要考虑两个关键的采样问题：中间点x_t的采样和条件向量场的设计。&lt;/p&gt;
&lt;h5 id=&#34;中间点-x_t-的采样&#34;&gt;中间点 $x_t$ 的采样&lt;/h5&gt;
&lt;p&gt;对于损失函数中的x_t，最常用的采样方式是线性插值：&lt;/p&gt;
&lt;p&gt;$ x_t = (1-t)x_0 + tx_1 $&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;t是从[0,1]均匀采样的时间点&lt;/li&gt;
&lt;li&gt;$x_0$ 是从初始分布（如标准正态分布）采样的点&lt;/li&gt;
&lt;li&gt;$x_1$ 是目标数据点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种采样方式的优势在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;保证了在 t=0 时得到 $x_0$，在 t=1 时得到 $x_1$&lt;/li&gt;
&lt;li&gt;提供了一条直观的线性轨迹&lt;/li&gt;
&lt;li&gt;计算简单且数值稳定&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;条件向量场的设计&#34;&gt;条件向量场的设计&lt;/h5&gt;
&lt;p&gt;在实际实现中，条件向量场 $u_t(x|x_1)$ 是可以自定义的。最简单和常用的设计是线性插值：&lt;/p&gt;
&lt;p&gt;$ u_t(x|x_1) = x_1 - x $&lt;/p&gt;
&lt;p&gt;这种设计的物理含义是：在时间 $t$ 处的点 $x$，其运动方向应该指向目标点 $x_1$。这样设计的好处是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;简单直观：向量场直接指向目标位置&lt;/li&gt;
&lt;li&gt;保证收敛：如果完美地学习了这个向量场，那么轨迹终点一定是目标点 $x_1$&lt;/li&gt;
&lt;li&gt;计算高效：不需要复杂的计算就能得到向量场的值&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;除了线性插值，还可以设计其他形式的条件向量场，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加入噪声的线性插值：$u_t(x|x_1) = x_1 - x + \epsilon$，其中$\epsilon$是随机噪声&lt;/li&gt;
&lt;li&gt;基于距离的加权：$u_t(x|x_1) = w(t)(x_1 - x)$，其中$w(t)$是时间相关的权重函数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;选择合适的条件向量场对模型的性能有重要影响。通常来说，向量场应该满足：从初始分布能够平滑地演化到目标分布，且路径应该尽可能简单和稳定。&lt;/p&gt;
&lt;h2 id=&#34;3-flow-matching-vs-diffusion&#34;&gt;3. Flow Matching vs Diffusion&lt;/h2&gt;
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/image-20250126184904307.png&#34; alt=&#34;image-20250126184904307&#34; style=&#34;zoom:30%; display: block; margin-left: auto; margin-right: auto;&#34; /&gt;
&lt;h4 id=&#34;reference&#34;&gt;Reference&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=7cMzfkWFWhI&amp;t=916s&#34;&gt;Flow Matching | Explanation + PyTorch Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=DDq_pIfHqLs&#34;&gt;How I Understand Flow Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2210.02747&#34;&gt;Flow Matching for Generative Modeling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>写作 &amp; 自媒体 &amp; 事业规划</title>
      <link>http://localhost:1313/blog/%E5%86%99%E4%BD%9C%E8%87%AA%E5%AA%92%E4%BD%93%E4%BA%8B%E4%B8%9A%E8%A7%84%E5%88%92/</link>
      <pubDate>Mon, 06 Jan 2025 16:37:14 +0800</pubDate>
      
      <guid>http://localhost:1313/blog/%E5%86%99%E4%BD%9C%E8%87%AA%E5%AA%92%E4%BD%93%E4%BA%8B%E4%B8%9A%E8%A7%84%E5%88%92/</guid>
      <description>&lt;p&gt;上周六晚上临睡前刷到了马丁更新的公众号，&lt;a href=&#34;https://mp.weixin.qq.com/s/uPHMQ2qwxmELtnzBDDcgdg&#34;&gt;构建你的黄金循环：写作/自媒体/事业成长&lt;/a&gt;。看了三遍，睡意全无。以至于今天上班路上回想起来，后劲依然很大。&lt;/p&gt;
&lt;p&gt;正好今年也要准备秋招了，写点东西聊聊这篇文章给我的启发，以及自己的事业规划。&lt;/p&gt;
&lt;h2 id=&#34;开始写作&#34;&gt;开始写作&lt;/h2&gt;
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/BB5747B3DF6917364BEA2AFAE0C6392D.jpg&#34; alt=&#34;BB5747B3DF6917364BEA2AFAE0C6392D&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;手绘了一张框架图，试图总结马丁这篇文章的核心观点，也就是如何实现个人黄金循环？&lt;/p&gt;
&lt;p&gt;在一个黄金循环里，首先需要明确自己的主事业。我是非常认可这一点的，选择适合自己的战场极为关键。为此通常需要思考很多问题，比如：这个赛道有多少人？你的核心优势在哪？这个赛道未来的发展趋势如何？当然最最主要的，&lt;strong&gt;你是否真的喜欢你的事业？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;能够成为主事业的，同时需要具备两个特质。其一，个人能力可以在进行事业的过程中得到提升，包括但不限于专业能力、沟通能力等等；其二，主事业可以让你获得一些资源，能够创造职业发展的机会和空间。&lt;/p&gt;
&lt;p&gt;能力提升和获得资源听起来似乎很解耦，尤其是纯粹的技术人员，可能工作了很多年，专业技术水平上升了，但是并没有获得资源，错过了很多的发展机会。对此，马丁给出了很好的解决方案，那就是写作。&lt;/p&gt;
&lt;h3 id=&#34;写作之于能力提升&#34;&gt;写作之于能力提升&lt;/h3&gt;
&lt;p&gt;多数人学习的时候，大量时间都花在了知识注入上，比如大量地查资料、看文章。结果就是知识都是零零散散的，没有形成体系。这时候花点时间进行写作就极为重要。通过写作，将吸收的知识揉碎并重新消化，从而构建起自己的知识体系框架。&lt;/p&gt;
&lt;p&gt;在日后的学习工作中，还可以通过不断迭代优化，使得自己的知识框架变得更加清晰鲁棒。个人能力也会在这个过程中，自然而然地上一个新的台阶。&lt;/p&gt;
&lt;h4 id=&#34;如何写作&#34;&gt;如何写作&lt;/h4&gt;
&lt;p&gt;这里不讲写作技巧，只谈写作习惯。主要有三点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;经常写，养成写作的习惯。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;大胆写，公开发表，接受大众评判。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;站在读者的角度，思考别人希望看到什么样的内容，想知道什么。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;写作之于资源获取&#34;&gt;写作之于资源获取&lt;/h3&gt;
&lt;p&gt;能力提升到一定程度后，写作质量也会跟着上升。&lt;/p&gt;
&lt;p&gt;高质量的内容输出可以帮助自己找到合适的圈子，并被关键人群认可，从而获得在圈子中的影响力。&lt;/p&gt;
&lt;h4 id=&#34;如何通过自媒体扩大影响力&#34;&gt;如何通过自媒体扩大影响力&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;自己牛逼还不够，需要让别人知道你有多厉害。&lt;/strong&gt; 在移动互联网时代，随着小红书、抖音这样的社交媒体兴起，写作的更具象化表达何尝不是打造个人IP？&lt;/p&gt;
&lt;p&gt;如果把个人IP当作一个产品来运营，它能带来的收益基本遵循一个公式：需求 x 商业 x 流量。&lt;/p&gt;
&lt;p&gt;首先是需求分析：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;当前多少人需求这样一个账号？这个需求会扩大还是缩小？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多少账号在这个赛道上，头部表现如何？中腰部表现如何？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;竞争优势在哪？&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其次是商业分析：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;运营账号的成本是多少？占用多少时间和精力？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果账号做到一定规模，商业价值是多少？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;赛道能不能接单？有没有其他变现可能，比如咨询、卖课、卖工具？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;非商业价值的东西能带来什么？比如特定行业、人群的影响力？&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后是流量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;目标人群在哪个平台？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;平台的规则是什么？鼓励什么内容？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;平台喜欢的内容风格是什么？样式、调性？&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;事业规划&#34;&gt;事业规划&lt;/h2&gt;
&lt;p&gt;在大模型这波热潮的推动下，一直从事NLP方向的同学在24年秋招都陆续拿到了不错的offer。但是反观其他的研究方向，就没有那么乐观了。老实说，这样的形势对我而言不得不慎重考虑自己的职业发展问题。&lt;/p&gt;
&lt;h3 id=&#34;我的经历&#34;&gt;我的经历&lt;/h3&gt;
&lt;h4 id=&#34;初识ai&#34;&gt;初识AI&lt;/h4&gt;
&lt;p&gt;我是21年下半年来到百度实习，当时所在的团队主要做人力资源数据的分析。一直到23年，很长一段时间我的研究都是围绕 HR 数据。比如我最早的&lt;a href=&#34;https://scholar.google.com/citations?user=mS9MgwsAAAAJ&amp;hl=en&#34;&gt;两篇学术论文&lt;/a&gt;，就是通过图神经网络来分析劳动力市场中的人才流动行为。&lt;/p&gt;
&lt;p&gt;这些研究站在计算机和社会科学的交叉点上，学术价值还不错（很幸运地被IEEE Trans.和CCF-A期刊收录）。但说实话，在实际落地时遇到了不少挑战。受限于数据和架构等因素，很难真正部署到生产环境。而且应用场景比较窄，主要局限在岗位推荐这样的领域。现在回想起来，虽然这些工作有其价值，但商业前景确实有限。说白了，没有客户愿意为这项技术买单。&lt;/p&gt;
&lt;h4 id=&#34;调整方向&#34;&gt;调整方向&lt;/h4&gt;
&lt;p&gt;由于一些原因，我在23年中转到了百度技术研究院。因为公司整体的战略调整，此时研究院的工作导向已经不再是追求发表学术论文，而是更注重技术支持业务。这时候我也开始意识到：还是得做一些真正能落地的研究。&lt;/p&gt;
&lt;p&gt;正好当时组里在和百度地图合作，一个棘手的问题就是真实场景数据缺失严重，导致学术圈的SOTA模型在做流量预测的时候鲁棒性较差。有了这样的机会，我就开始动手设计了一套算法，通过蒸馏多个数据集的知识来提高单区域预测精度。在实际应用场景上取得一些效果后，相关研究成果也被 CCF-A 会议 ICDE 所认可接收。但此时我发现，中论文带来的多巴胺已经极低了。说到底不过是审稿人认可的几张纸罢了，它距离被大众所认可还很远。&lt;/p&gt;
&lt;h4 id=&#34;拥抱大模型&#34;&gt;拥抱大模型&lt;/h4&gt;
&lt;p&gt;转眼到了24年，大模型开始如日中天。不管是学界还是业界，不管是哪个领域，几乎都给自己的方向加了&amp;quot;LLM+&amp;ldquo;这样一个前缀，百度也不例外。这一年的大多数时间，我都在学习LLM，并且参与到两个垂类项目中。一个是大语言模型驱动的对话系统，另一个是导航场景下基于LLM的API调用。&lt;/p&gt;
&lt;p&gt;虽然没有什么学术论文的产出，但是我的实战经验在这个过程中确是进步了不少。不仅仅是在写代码的层面，更多的是学会从宏观层面思考问题。因为一个项目通常会涉及多个方面的点，学术问题通常是针对单个点进行突破，这时候只需要思考&amp;quot;怎么做&amp;quot;就可以，但是在具体的应用项目里，首先需要思考的是&amp;quot;做什么&amp;rdquo;？这在没有前车之鉴的时候尤为重要，好的行动方案往往可以事半功倍，而这些都需要积累经验。&lt;/p&gt;
&lt;h4 id=&#34;新的机遇&#34;&gt;新的机遇&lt;/h4&gt;
&lt;p&gt;这样的工作状态大约持续到了24年的11月份，组里开展了一些预训练基座模型的训练。考虑到之前的一些工作经验，我这边承接了语音大模型方向的探索。这是个很重的研究课题，通常都是多人团队在推进，对我个人来讲，不得不承认这是一个自不量力的选择，但同时也是一个很好的机会。相比于之前的研究方向，语音领域的想象空间要大得多，是个更为宽广的赛道。&lt;/p&gt;
&lt;h3 id=&#34;我的黄金循环&#34;&gt;我的黄金循环&lt;/h3&gt;
&lt;p&gt;看了马丁的文章，反思自己这些年的学习经历，我也给自己的2025年设计下面这样一个黄金循环：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主线事业&lt;/strong&gt;  语音生成/音乐生成 | 这个事业需要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;做出至少一个代表性的研究工作或者相关产品；（能力提升）&lt;/li&gt;
&lt;li&gt;提高音频领域的影响力，可以通过内容创作，开源项目实现；（获得资源）&lt;/li&gt;
&lt;li&gt;和该领域的人构建联系，物色优秀的交流对象；（获得资源）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;写作内容&lt;/strong&gt;  我需要写：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;前沿的AI技术，包括通用技术和语音相关技术；&lt;/li&gt;
&lt;li&gt;实用的AI产品；&lt;/li&gt;
&lt;li&gt;好玩的应用落地；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;内容传播&lt;/strong&gt;  我会通过以下途径传播：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;个人博客&lt;/li&gt;
&lt;li&gt;微信公众号&lt;/li&gt;
&lt;li&gt;小红书&lt;/li&gt;
&lt;li&gt;X&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相比内容更新速度，更关注创作的质量和深度，警惕成为AI日报。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>2025年的一些计划（上）</title>
      <link>http://localhost:1313/blog/2025%E5%B9%B4%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%A1%E5%88%92/</link>
      <pubDate>Thu, 02 Jan 2025 22:54:38 +0800</pubDate>
      
      <guid>http://localhost:1313/blog/2025%E5%B9%B4%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%A1%E5%88%92/</guid>
      <description>&lt;p&gt;博客很久没更新了。原计划能周更，没想到自从8月份建站以来，一转眼就是新的一年，也是该写点东西了。&lt;/p&gt;
&lt;p&gt;本来想规划2025年的年度计划，盘算了下，一年这个尺度还是太长了，容易没有紧凑感。&lt;/p&gt;
&lt;p&gt;所以，在这个相隔许久的博客里，做一下2025的上半年规划吧😊&lt;/p&gt;
&lt;h3 id=&#34;工作方面&#34;&gt;工作方面&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;每周更新一次博客&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;坚持写代码，提交到GitHub&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;尝试做一些开源项目，半年内至少开源三个小项目&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;坚持每天更新 X 和小红书，X 能做到100个粉丝&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;健康方面&#34;&gt;健康方面&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;空腹体重增加 2kg&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;每周坚持三大项训练&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;半年后进行全面体检&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;学习方面&#34;&gt;学习方面&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;提高英语听力和口语水平&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（探索）录纯英文的 vlog&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;平均每个月能阅读一本书&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;平均每天刷一道 leetcode&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;减法&#34;&gt;减法&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;减少周末睡懒觉的时间&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;减少周末打游戏刷剧的时间&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;减少工作间隙休息频率&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>如何让大语言模型听到声音(一)</title>
      <link>http://localhost:1313/blog/%E5%A6%82%E4%BD%95%E8%AE%A9%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%90%AC%E5%88%B0%E5%A3%B0%E9%9F%B3/</link>
      <pubDate>Thu, 08 Aug 2024 16:48:01 +0800</pubDate>
      
      <guid>http://localhost:1313/blog/%E5%A6%82%E4%BD%95%E8%AE%A9%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%90%AC%E5%88%B0%E5%A3%B0%E9%9F%B3/</guid>
      <description>&lt;p&gt;本文介绍了音频数据的基本概念、音频信号的预处理流程、音频相关任务以及在深度学习领域，处理音频信号的两种常见架构。&lt;/p&gt;
&lt;h2 id=&#34;了解音频数据&#34;&gt;了解音频数据&lt;/h2&gt;
&lt;h3 id=&#34;1-频率--振幅--位深&#34;&gt;1. 频率 &amp;amp; 振幅 &amp;amp; 位深&lt;/h3&gt;
&lt;p&gt;声音的本质是连续信号，要想让物理设备捕获这种连续值，则需要通过采样的方式。这一过程通常涉及几个关键术语：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;采样率：即一秒钟内进行采样的频率，以赫兹 (Hz) 为单位。根据奈奎斯特极限，从信号中能够捕获的最高频率正好是采样率的一半。对人类而言，声音的可听频率低于 8kHz，更高频的声音信号人耳是听不到的。因此，16 kHz 的采样率对于处理音频而言足矣。但是如果采样率低于 8kHz，就会丢失一些信息，导致声音变得沉闷。如果训练样本中的音频频率不一致，则需要引入&lt;strong&gt;重采样&lt;/strong&gt;(resampling)的预处理步骤来确保采样频率的一致性。&lt;/li&gt;
&lt;li&gt;振幅：声音是由人类可听频率内的气压变化产生的，其中声压的级别用振幅表示，以分贝 (dB) 为单位。例如，低于 60dB 人耳就很难感知到。&lt;/li&gt;
&lt;li&gt;位深：刻画振幅值的度量精度。常见的位深有 16-bit 和 24-bit，这些都是整数样本，浮点数样本是 32-bit。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-波形--频谱--光谱&#34;&gt;2. 波形 &amp;amp; 频谱 &amp;amp; 光谱&lt;/h3&gt;
&lt;p&gt;波形图是基于采样点上振幅值所表示的曲线图，横轴是时间，纵轴是振幅值。它记录了音频信号的强度随时间变化的特征。&lt;/p&gt;
&lt;div class=&#34;image-container&#34;&gt;
    &lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/image-20240805190849457.png&#34; alt=&#34;波形示例&#34; style=&#34;zoom:50%;&#34; /&gt;
    &lt;div class=&#34;image-caption&#34;&gt;一段波形示例&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;频谱是波形数据经过离散傅立叶变换得到的，横轴是频率，纵轴是振幅值。它记录了音频信号的强度随频率变化的特征。&lt;/p&gt;
&lt;div class=&#34;image-container&#34;&gt;
  &lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/image-20240805191047605.png&#34; alt=&#34;image-20240805191047605&#34; style=&#34;zoom:50%;&#34; /&gt;
  &lt;div class=&#34;image-caption&#34;&gt;一段频谱示例&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;波形图和频谱图刻画的是振幅在时间/频率维度的变化，光谱图则表示频率在时间维度上的变化。它的基本思想是对时间进行微分，在每个极短的时间窗口内进行傅立叶变化，得到该时刻的频谱，最后将所有时刻的频谱拼接起来，振幅值大小用颜色深浅表示。&lt;/p&gt;
&lt;div class=&#34;image-container&#34;&gt;
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/image-20240805191528713.png&#34; alt=&#34;image-20240805191528713&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;div class=&#34;image-caption&#34;&gt;频谱示例&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;人类的听觉系统对较低频率的变化比对较高频率的变化更敏感，这种灵敏度随着频率的增加而呈对数下降。梅尔谱图考虑到这一特性，对频率进行梅尔滤波。在 Whisper 中的输入就是梅尔谱图，每个样本的大小是 [频率维度，时间帧数]。如果采用 80 个 Mel 滤波器，并将音频信号划分为 3000 帧，那么梅尔谱图的特征大小就是 [80, 3000]。&lt;/p&gt;
&lt;div class=&#34;image-container&#34;&gt;
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/image-20240805191814452.png&#34; alt=&#34;image-20240805191814452&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;div class=&#34;image-caption&#34;&gt;梅尔谱图示例&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;梅尔光谱图可以捕捉到对人类感知而言更有意义的信号，因此在语音识别、音色分类等任务中应用较广。但相比于标准光谱图，梅尔光谱图由于引入滤波操作，会导致信号的过滤，使得从梅尔光谱图转换回波形图变得比较棘手，需要引入 HiFiGAN 这种模型来解决。&lt;/p&gt;
&lt;h2 id=&#34;音频信号的预处理流程&#34;&gt;音频信号的预处理流程&lt;/h2&gt;
&lt;h3 id=&#34;第一步重采样&#34;&gt;第一步：重采样&lt;/h3&gt;
&lt;p&gt;大多数深度学习模型都是基于 16 kHz 采样的音频信号进行训练的，为了和这些预训练模型保持一致，我们首先需要对自己的数据集进行重采样（如果原始数据采样频率不是 16 kHz）。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; datasets &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Audio
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;minds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; minds&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cast_column(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;audio&amp;#34;&lt;/span&gt;, Audio(sampling_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16_000&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;下面两张图是采样前后的直观对比，可以看到将 8 kHz 重采样到 16 kHz 之后多了更多的样本点。&lt;/p&gt;
&lt;div class=&#34;image-container&#34;&gt;
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/image-20240805195059588.png&#34; alt=&#34;image-20240805195059588&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;div class=&#34;image-caption&#34;&gt;8 kHz 采样片段&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;image-container&#34;&gt;
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/image-20240805195114381.png&#34; alt=&#34;image-20240805195114381&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;div class=&#34;image-caption&#34;&gt;16 kHz 采样片段&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;第二步过滤时长较长的数据&#34;&gt;第二步：过滤时长较长的数据&lt;/h3&gt;
&lt;p&gt;为了防止推理或者训练的时候内存不足，可以限制数据的时长，将时长超过一定阈值的样本从原始数据中删掉。&lt;/p&gt;
&lt;h3 id=&#34;第三步特征提取&#34;&gt;第三步：特征提取&lt;/h3&gt;
&lt;p&gt;原始音频数据只有振幅波形，还需要从中抽取更丰富的特征用于模型训练。以 Whisper 为例，其特征提取包含两个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;填充/截断，控制每个样本长度都是 30 秒&lt;/li&gt;
&lt;li&gt;将音频矩阵转换为对数梅尔光谱图作为输入特征&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;音频相关的任务&#34;&gt;音频相关的任务&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;音频分类：歌曲识别&lt;/li&gt;
&lt;li&gt;自动语音识别 (ASR)：将讲者说话的内容自动转录成文字&lt;/li&gt;
&lt;li&gt;声纹分割 (speaker diarization)：将播客音频中不同讲者的音频分离开来&lt;/li&gt;
&lt;li&gt;Text to Speech：将文字转录为人声&lt;/li&gt;
&lt;li&gt;Voice Conversation：输入和输出都是音频&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;用于处理音频信号的-transformer&#34;&gt;用于处理音频信号的 Transformer&lt;/h2&gt;
&lt;div class=&#34;image-container&#34;&gt;
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/transformers_blocks.png&#34; alt=&#34;The transformer with audio input and output&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;div class=&#34;image-caption&#34;&gt;语音作为输入和输出的Transformer结构&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;模型的输入可以是文本、波形、频谱。对于波形和频谱的输入，可以采用 CNN 来作为特征提取器得到初始表征。模型的输出一般是文本或者频谱，如果想得到波形则可以对频谱进行逆变换。&lt;/p&gt;
&lt;h3 id=&#34;ctc-架构&#34;&gt;CTC 架构&lt;/h3&gt;
&lt;p&gt;CTC 全称是 Connectionist Temporal Classification，直译过来就是联结主义时序分类，这是用于训练早期语音模型的损失函数。比如 Wav2Vec2.0 就采用该损失用于下游语音识别任务的微调。在 Wav2Vec2.0 中，模型采用 Temporal Convolution 作为特征抽取器，Transformer 作为编码器。&lt;/p&gt;
&lt;div class=&#34;image-container&#34;&gt;
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/image-20240806111914404.png&#34; alt=&#34;image-20240806111914404&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;div class=&#34;image-caption&#34;&gt;Wav2Vec 2.0&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;具体而言，Wav2Vec2.0 首先用卷积层将初始的音频信号离散化，得到初始的音频表征 $\mathcal{Z}$，注意这里采用的是有 overlap 的滑移窗口。表征 $\mathcal{Z}$ 随后有两个去处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入 Transformer 编码器得到语义表征 $\mathcal{C}$。&lt;/li&gt;
&lt;li&gt;经过量化得到量化表征 $\mathcal{Q}$。这里的量化表征可以看作对连续的 $d$ 维表征空间进行离散化，它首先预设有 $G$ 个 $d/G$ 维的子空间，每个子空间里有 $V$ 个条目 (entries)，音频表征 $z \in \mathbb{R}^{d}$ 可以转换为 $G$ 个子空间中原型向量 $e \in \mathbb{R}^{V \times d/G}$ 的组合。基于这一思想，我们可以对初始表征 $z$ 进行映射变换得到索引矩阵 $\mathcal{I} \in \mathbb{R}^{G \times V}$，随后将 $G$ 个子空间中索引概率最大的那一个条目 $i = \text{argmax}\_{j} p\_{g,j}$ 拿出来拼接并进行线性变换，得到量化表征 $q \in \mathbb{R}^{f}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;这里对比学习不是作用于 $\mathcal{Z}$ 和 $\mathcal{C}$ ，而是引入 product quantization 操作得到量化表征 $\mathcal{Q}$。可以将量化表征看作是 nn.Embedding，它是有限向量的集合，因为量化表征是 G 个子空间有限条目的线性变换而来，因此它也是离散的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在预训练阶段，Wav2Vec2.0 采用语义表征 $\mathcal{C}$ 与量化表征 $\mathcal{Q}$ 之间的对比学习损失和量化表征 $\mathcal{Q}$ 的熵作为损失函数。在微调阶段，Wav2Vec2.0 采用 CTC 损失。假设这里的音频对应文本是 &amp;ldquo;hello&amp;rdquo;，经过卷积后得到 20 个 token，这显然和目标文本（5 个字母）不对齐，因此需要用 CTC 损失。在 CTC 中，我们考虑 20 个 token 预测值的排列组合，得到所有这 5 个字母按照 &amp;ldquo;h -&amp;gt; e -&amp;gt; l -&amp;gt; l -&amp;gt; o&amp;rdquo; 顺序构成的序列，计算它们的概率和。微调阶段就是希望这个概率最大，比如这个例子中预测值可能是 &amp;ldquo;hhheeeeelllllllllloo&amp;rdquo;，这时候再进行解码即可（连续字母去重）。&lt;/p&gt;
&lt;h3 id=&#34;seq2seq-架构&#34;&gt;Seq2Seq 架构&lt;/h3&gt;
&lt;p&gt;在 CTC 架构中，输入和输出都是相同的长度，因此在进行 ASR 等任务时需要引入 CTC 来实现文本对齐。但是在 Seq2Seq 架构中，因为模型采用生成式理念，因此输入和输出的长度可以不一致。比如下面 Whisper 架构：&lt;/p&gt;
&lt;div class=&#34;image-container&#34;&gt;
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/zruiii/storage.zruiii.com@main/images/whisper_architecture.svg&#34; alt=&#34;Whisper is a transformer encoder-decoder model&#34; style=&#34;zoom:60%;&#34; /&gt;
&lt;div class=&#34;image-caption&#34;&gt;Whisper&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;在 Whisper 中，输入是音频信号转换的梅尔频谱图，输出是文本。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>